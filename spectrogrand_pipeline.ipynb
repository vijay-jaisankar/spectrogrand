{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Spectrogrand: Text to AudioVisual Pipeline"]},{"cell_type":"markdown","metadata":{},"source":["## Install dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":false,"trusted":true},"outputs":[],"source":["!pip install --upgrade -r /kaggle/input/spectrogrand-public-release/kaggle-public-release/REQUIREMENTS.txt"]},{"cell_type":"markdown","metadata":{},"source":["## Create appropriate directories for outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import time\n","\n","current_timestamp = str(int(time.time()))\n","BASE_DIR = f\"./logs_{current_timestamp}\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!mkdir -p {BASE_DIR}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["AUDIO_STREAM_DIR = f\"{BASE_DIR}/saved_audio_stream\"\n","IMAGE_PARENT_DIR_SPEC = f\"{BASE_DIR}/saved_image_parent_spec\"\n","IMAGE_STREAM_DIR_SPEC = f\"{BASE_DIR}/saved_image_stream_spec\"\n","IMAGE_STREAM_DIR_SD = f\"{BASE_DIR}/saved_image_stream_sd\"\n","IMAGE_STREAM_DIR_NST_1 = f\"{BASE_DIR}/saved_image_stream_nst_1\"\n","IMAGE_STREAM_DIR_NST_2 = f\"{BASE_DIR}/saved_image_stream_nst_2\"\n","\n","!mkdir -p {AUDIO_STREAM_DIR}\n","!mkdir -p {IMAGE_PARENT_DIR_SPEC}\n","!mkdir -p {IMAGE_STREAM_DIR_SPEC}\n","!mkdir -p {IMAGE_STREAM_DIR_SD}\n","!mkdir -p {IMAGE_STREAM_DIR_NST_1}\n","!mkdir -p {IMAGE_STREAM_DIR_NST_2}"]},{"cell_type":"markdown","metadata":{},"source":["## Initialise the models and helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","    Collection of helper functions pertaining to the audio domain of spectrogrand\n","\"\"\"\n","from diffusers import AudioLDM2Pipeline\n","from typing import Optional, List\n","import scipy\n","import numpy as np\n","import librosa\n","import pickle\n","\n","import torch\n","torch.random.manual_seed(42)\n","DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","from transformers import ClapModel, ClapProcessor\n","\n","\n","# Load the AudioLDM pipeline\n","audio_ldm_pipeline = AudioLDM2Pipeline.from_pretrained(\"cvssp/audioldm2-music\")\n","audio_ldm_pipeline.to(DEVICE)\n","\n","# Load the CLAP pipeline\n","clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-fused\")\n","clap_model.to(DEVICE)\n","clap_processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n","\n","\"\"\"\n","    @method create_and_save_audio_file\n","        Use the `audioldm2-music` model to generate synthetic audio conditioned on inputs\n","    @param text_prompt: Descriptor of the audio to be generated (@note The user is requested to be as verbose as possible)\n","    @param output_file_path: Path to which the generated audio is to saved\n","    @param num_inference_steps: Number of inference steps for the audioldm2-music model (@note The higher this value, the longer the po)\n","    @param audio_length: Length of the audio piece (in s) (default: 10.0)\n","    @param negative_prompt: Negative prompt to be passed to the audioldm2-music model (default: 'low quality, monotonous, boring')\n","\"\"\"\n","def create_and_save_audio_file(text_prompt:str, output_file_path:str, num_inference_steps:int=500, audio_length:float=10.0, negative_prompt = \"low quality, monotonous, boring\") -> Optional[str]:\n","    try:\n","        global audio_ldm_pipeline\n","        # Generate audio\n","        audio = audio_ldm_pipeline(\n","            text_prompt,\n","            negative_prompt=negative_prompt,\n","            num_inference_steps=num_inference_steps,\n","            audio_length_in_s=float(audio_length),\n","            num_waveforms_per_prompt=2\n","        ).audios[0]\n","        # Save audio\n","        scipy.io.wavfile.write(output_file_path, rate=16000, data=audio)\n","        return output_file_path\n","    except Exception as e:\n","        print(f\"Error while generating and saving audio: {e}\")\n","        return None\n","\n","\"\"\"\n","    @method load_wav_file\n","        Load a wav file from a given path\n","    @param input_file_path: Path containing the input audio file\n","\"\"\"\n","def load_wav_file(input_file_path:str):\n","    try:\n","        sr, data = scipy.io.wavfile.read(input_file_path)\n","        return sr, data\n","    except Exception as e:\n","        print(f\"Error while reading {input_file_path}: {e}\")\n","        return None, None\n","    \n","\"\"\"\n","    @method resample_audio_data\n","        Resample audio data to a target sampling rate\n","    @param origin_data: Numpy array containing the audio data (@note Intended inputs stem from the `load_wav_file` method)\n","    @param origin_sampling_rate: Sampling rate of the input audio (@note Intended inputs stem from the `load_wav_file` method)\n","    @param new_sampling_rate: Desired sampling rate (default: 48000)\n","\"\"\"\n","def resample_audio_data(origin_data:np.ndarray, origin_sampling_rate:int, new_sampling_rate:int=48000) -> Optional[np.ndarray]:\n","    try:\n","        origin_type = origin_data.dtype\n","        resampled_data = librosa.resample(origin_data.T.astype('float'), orig_sr = origin_sampling_rate, target_sr = new_sampling_rate) \n","        resampled_data = librosa.to_mono(resampled_data)        \n","        resampled_data = resampled_data.T.astype(origin_type)\n","        data_np = np.array(resampled_data)\n","        return data_np\n","    except Exception as e:\n","        print(f\"Error while resampling audio data: {e}\")\n","        return None\n","\n","\"\"\"\n","    @method compute_clap_embeddings\n","        Compute CLAP embeddings for an input audio file\n","    @input input_file_path: Path to the input audio file\n","\"\"\"\n","def compute_clap_embeddings(input_file_path:str) -> Optional[torch.Tensor]:\n","    try:\n","        global clap_processor, clap_model, DEVICE\n","        # Load audio and resample to 48000 Hz\n","        sr, origin_data = load_wav_file(input_file_path=input_file_path)\n","        origin_data_resampled = resample_audio_data(origin_data=origin_data, origin_sampling_rate=sr, new_sampling_rate=48000)\n","        # Get CLAP outputs\n","        clap_inputs = clap_processor(audios=origin_data_resampled, sampling_rate=48000, return_tensors=\"pt\").to(DEVICE)\n","        clap_outputs = clap_model.get_audio_features(**clap_inputs)\n","        audio_embeds = clap_outputs[0].detach().cpu()\n","        return audio_embeds\n","    except Exception as e:\n","        print(f\"Error while computing CLAP embeddings for {input_file_path}: {e}\")\n","        return None\n","    \n","\"\"\"\n","    @method compute_clap_similarity\n","        Compute CLAP similarity for an input audio file with respect to a saved ground truth mapping of embeddings\n","        @input input_file_path: Path to the input audio file\n","        @input ground_truth_dict_path: Path to the mapping .pkl file \n","        @note The ground truth mapping should be a .pkl file with the following schema:\n","            {\n","                \"genre_name\" : [list_of_clap_embeddings],\n","                ...\n","            }\n","        @input filter_genre: Genre name to compute from. If values are to be aggregated across the entire search space, this value should be left as `None`. (default: None)\n","\"\"\"\n","def compute_clap_similarity(input_file_path:str, ground_truth_dict_path:str, filter_genre:Optional[str]=None) -> Optional[float]:\n","    try:\n","        # Load the embeddings from the ground truth mapping and set the search space\n","        with open(ground_truth_dict_path, \"rb\") as f:\n","            data = pickle.load(f)\n","        input_search_space_embeds = []\n","        if filter_genre is not None:\n","            # Convert `filter_genre` into underscore format if required\n","            if \"_\" not in filter_genre: # eg: 'bass house'\n","                filter_genre = filter_genre.replace(\" \",\"_\")\n","            input_search_space_embeds = data[filter_genre]\n","        else:\n","            for _k in data:\n","                input_search_space_embeds.extend(data[_k])\n","        assert len(input_search_space_embeds) >= 1\n","\n","        # Compute CLAP embeddings for the input file\n","        source_embed = compute_clap_embeddings(input_file_path=input_file_path)\n","\n","        # Keep track of running dot product scores\n","        running_score = 0.0\n","        for target_embed in input_search_space_embeds:\n","            z = source_embed@target_embed.T\n","            running_score += float(z.detach().cpu())\n","\n","        # Return the average dot product score\n","        return (running_score)/float(len(input_search_space_embeds))\n","    except Exception as e:\n","        print(f\"Error while computing CLAP similarity score for {input_file_path}: {e}\")\n","        return None\n","    \n","\"\"\"\n","    @method load_wav_chunk\n","        Load a time-defined chunk of audio from a wav file\n","    @param input_file_path: Path to the input wav file\n","    @param chunk_offset: Timestamp (in s) from which the chunk starts\n","    @param chunk_duration: Duration of the returned chunk (in s) (default: 0.1)\n","\"\"\"\n","def load_wav_chunk(input_file_path:str, chunk_offset:float, chunk_duration:float=0.1):\n","    try:\n","        y, sr = librosa.load(path=input_file_path, offset=float(chunk_offset), duration=float(chunk_duration), sr=None)\n","        return sr, y\n","    except Exception as e:\n","        print(f\"Error while loading chunk from {input_file_path}: {e}\")\n","        return None, None\n","    \n","\"\"\"\n","    @method create_and_save_audio_file_stream\n","        Use the `audioldm2-music` model to generate synthetic audio conditioned on inputs\n","    @param topic: Topic of the audio files to be generated\n","    @param output_dir: Directory to which the generated audio files are to saved\n","    @param num_inference_steps_list: List containing the number of inference steps (default: [500, 750])\n","    @param audio_length: Length of the audio piece (in s) (default: 10.0)\n","    @param negative_prompt: Negative prompt to be passed to the audioldm2-music model (default: 'low quality, monotonous, boring')\n","\"\"\"\n","def create_and_save_audio_file_stream(topic:str, output_dir:str, num_inference_steps_list:list=[500, 750], audio_length:float=10.0, negative_prompt = \"low quality, monotonous, boring\") -> Optional[List[str]]:\n","    try:\n","        global audio_ldm_pipeline\n","        # Generate audio\n","        # Keep running count of the current time index and number of images generated\n","        num_audios_generated = 0\n","        saved_output_file_names = []\n","        \n","        # Construct the text prompt\n","        text_prompt = f\"Jumpy electronic house music for {topic}\"\n","\n","        for num_infer in num_inference_steps_list:\n","            output_file = f\"{output_dir}/audio{num_audios_generated}.wav\"\n","            output_file = create_and_save_audio_file(text_prompt=text_prompt, output_file_path=output_file, num_inference_steps=num_infer, audio_length=audio_length, negative_prompt=negative_prompt)\n","            if output_file is not None:\n","                saved_output_file_names.append(output_file)\n","                num_audios_generated += 1\n","\n","        return saved_output_file_names\n","    except Exception as e:\n","        print(f\"Error while generating and saving audio stream: {e}\")\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","    Collection of helper functions pertaining to the spectrogram domain of spectrogrand\n","\"\"\"\n","from typing import Optional, List\n","from PIL import Image\n","from io import BytesIO\n","import numpy as np\n","from glob import glob\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from torchvision import transforms\n","\n","torch.random.manual_seed(42)\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","\n","TORCH_DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","TF_DEVICE = \"/gpu:0\" if torch.cuda.is_available() else \"/cpu\"\n","# Use another GPU core for SDXL if possible\n","SD_DEVICE = \"cpu\"\n","if int(torch.cuda.device_count()) == 1:\n","    SD_DEVICE = \"cuda:0\"\n","else:\n","    SD_DEVICE = \"cuda:1\"\n","\n","from diffusers import DiffusionPipeline\n","\n","# @note These defaults can be changed based on the user's preferences\n","GENRE_COLOUR_MAPPING = {\n","    'future house' : [\"blue\", \"red\"],\n","    'bass house' : [\"black\", \"purple\"],\n","    'progressive house' : [\"orange\", \"yellow\"],\n","    'melodic house' : [\"green\", \"blue\"]\n","}\n","\n","GENRE_WORD_MAPPING = {\n","    'future house' : [\"unveils\", \"surprises\"],\n","    'bass house' : [\"ascends\", \"explodes\"],\n","    'progressive house' : [\"balances\", \"hypnotizes\"],\n","    'melodic house' : [\"blends\", \"stuns\"]\n","}\n","\n","\n","# Load the Stabke Duffusion XL pipeline\n","sdxl_pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n","sdxl_pipeline.to(SD_DEVICE)\n","\n","# Load the VILA pipeline\n","vila_model = hub.load('https://tfhub.dev/google/vila/image/1')\n","vila_predict_fn = vila_model.signatures['serving_default']\n","\n","# Load the Magenta pipeline\n","magenta_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n","\n","# Load the surprise estimation pipeline\n","class CreativeNet(nn.Module):\n","    def __init__(self, train_baseline_classifier = False, num_output_classes = 2, dropout_rate = 0.20):\n","        super().__init__()\n","        \n","        # Set instance variables\n","        self.train_baseline_classifier = train_baseline_classifier\n","        self.num_outuput_classes = num_output_classes\n","        self.dropout_rate = dropout_rate\n","        \n","        # Set the current device for tensor calculations\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        \n","        # Baseline: MobileNet V3 small\n","        self.baseline = models.mobilenet_v3_small(weights = models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n","        \n","        # Freeze the parameters of the base model (including but not limited to the last layers)\n","        for param in self.baseline.parameters():\n","            param.requires_grad = False\n","        \n","        if self.train_baseline_classifier:\n","            for param in self.baseline.classifier.parameters():\n","                param.requires_grad = True\n","                \n","        # Fully-connected block\n","        self.fc1 = nn.Linear(1000, 128)\n","        self.dropout1 = nn.Dropout(self.dropout_rate)\n","        self.fc2 = nn.Linear(128, 32)\n","        self.dropout2 = nn.Dropout(self.dropout_rate)\n","        self.fc3 = nn.Linear(32, self.num_outuput_classes)\n","        \n","    def forward(self, x):\n","        # Baseline\n","        x = x.to(self.device)\n","        x = self.baseline(x)\n","        \n","        # FC Block\n","        x = F.leaky_relu(self.fc1(x))\n","        x = self.dropout1(x)\n","        x = F.leaky_relu(self.fc2(x))\n","        x = self.dropout2(x)\n","        x = F.leaky_relu(self.fc3(x))\n","        x = torch.sigmoid(x)\n","        return x\n","    \n","surprise_model_args = {\n","        \"train_baseline_classifier\" : False, \n","        \"num_output_classes\" : 2,\n","        \"dropout_rate\" : 0.35\n","    }\n","surprise_model = CreativeNet(**surprise_model_args).to(TORCH_DEVICE)\n","\n","\"\"\"\n","    @method generate_and_save_image\n","        Generate and save an image for a text prompt \n","    @param prompt: Textual prompt containg the specifics of the image to be generated\n","    @param output_file_path: Path to the output file where the generated image is to be stored\n","    @param num_inference_steps: Number of diffusion steps SDXL will take to generate the image (@note The higher this number, the longer the pipeline will take while maintaining higher-quality outputs) (default: 50)\n","\"\"\"\n","def generate_and_save_image(prompt:str, output_file_path:str, num_inference_steps:int=50) -> Optional[str]:\n","    try:\n","        global sdxl_pipeline\n","        # Generate image\n","        images = sdxl_pipeline(prompt=prompt, num_inference_steps=num_inference_steps)\n","        img = images[0][0]\n","        # Save image\n","        img.save(output_file_path)\n","        return output_file_path\n","    except Exception as e:\n","        print(f\"Error while generating image: {e}\")\n","        return None\n","    \n","\"\"\"\n","    @method generate_and_save_image_stream\n","        Generate and save genre-driven candidate album covers\n","        @note To change the genre-mapping configs, rewrite GENRE_COLOR_MAPPING and/or GENRE_WORD_MAPPING before calling this function.\n","    @param genre_name: Name of the genre, as it appears in the keys of GENRE_COLOR_MAPPING and GENRE_WORD_MAPPING\n","    @param topic: Topic of the music piece\n","    @param output_dir: Path to the parent directory where the contiguous melspectrogram images are to be stored\n","    @param num_inference_steps: Number of diffusion steps SDXL will take to generate the image (@note The higher this number, the longer the pipeline will take while maintaining higher-quality outputs) (default: 50)\n","\"\"\"\n","def generate_and_save_image_stream(genre_name:str, topic:str, output_dir:str, num_inference_steps:int=50) -> Optional[List[str]]:\n","    try:\n","        global GENRE_COLOUR_MAPPING, GENRE_WORD_MAPPING\n","        # Keep running count of the current time index and number of images generated\n","        num_images_generated = 0\n","        saved_output_file_names = []\n","\n","        # Generate and save an image using the GridSearch heuristic\n","        for colour in GENRE_COLOUR_MAPPING[genre_name]:\n","            for word in GENRE_WORD_MAPPING[genre_name]:\n","                output_file = f\"{output_dir}/sdxl{num_images_generated}.png\"\n","                # Construct the prompt\n","                prompt = f\"{colour} colored album cover for music about {topic} that {word}\"\n","                output_file = generate_and_save_image(prompt=prompt,output_file_path=output_file,num_inference_steps=num_inference_steps)\n","                if output_file is not None:\n","                    saved_output_file_names.append(output_file)\n","                    num_images_generated += 1\n","        \n","        return saved_output_file_names\n","    except Exception as e:\n","        print(f\"Error while generating and saving image stream: {e}\")\n","        return None\n","\n","\"\"\"\n","    @method get_vila_score\n","        Score an image for its aesthetic qualities using the VILA model\n","    @param input_image_path: Path to the input image\n","\"\"\"\n","def get_vila_score(input_image_path:str) -> Optional[float]:\n","    try:\n","        global TF_DEVICE   , vila_predict_fn, vila_model \n","        # Load image\n","        img = Image.open(input_image_path)\n","        # Convert image to Bytes array @ref https://stackoverflow.com/a/33117447\n","        img_byte_arr = BytesIO()\n","        img.save(img_byte_arr, format=img.format)\n","        img_byte_arr = img_byte_arr.getvalue()\n","        # Get predictions\n","        with tf.device(TF_DEVICE):\n","            prediction = vila_predict_fn(tf.constant(img_byte_arr))\n","            return float(prediction['predictions'][0][0])\n","    except Exception as e:\n","        print(f\"Error while calculating VILA score for {input_image_path}: {e}\")\n","        return None\n","\n","\"\"\"\n","    @method get_surprise_score\n","        Get the surprise coefficient from a MUMU-trained model\n","    @param input_image_path: Path to the input SDXL image\n","    @param model_path: Path to the `.pt` file containing the surprise estimation model\n","    @note The model will be loaded with the `model.load_state_dict(torch.load(PATH))` moniker.\n","\"\"\"\n","def get_surprise_score(input_image_path:str, model_path:str) -> Optional[str]:\n","    try:\n","        global TORCH_DEVICE, surprise_model   \n","\n","        # Transform the input image into a torch tensor @ref: https://www.projectpro.io/recipes/convert-image-tensor-pytorch\n","        transform = transforms.Compose([\n","                transforms.Resize((256, 256)),\n","                transforms.ToTensor(),\n","        ])\n","\n","        img = Image.open(input_image_path).convert(\"RGB\")\n","        transformed_img = transform(img=img)\n","        x = torch.Tensor(transformed_img)\n","        x = x.to(TORCH_DEVICE)\n","\n","        # Load model\n","        surprise_model.load_state_dict(torch.load(model_path))\n","        surprise_model.to(TORCH_DEVICE)\n","        surprise_model.eval()\n","\n","        # Compute outputs\n","        with torch.no_grad():\n","            outputs = surprise_model(x.unsqueeze(0))\n","            y = torch.softmax(outputs, dim = 1).detach().cpu()\n","            selected_score = float(y[0][1].item()) # Order of scores: ai, human\n","        return selected_score\n","    except Exception as e:\n","        print(f\"Error while classifying genre of {input_image_path}: {e.with_traceback()}\")\n","        return None\n","    \n","\"\"\"\n","    @method neural_style_transfer_vanilla\n","        Perform fast neural style transfer using the Magenta model\n","        @note ref: https://www.kaggle.com/models/google/arbitrary-image-stylization-v1/frameworks/tensorFlow1/variations/256/versions/2?tfhub-redirect=true\n","    @param content_image_path: Path to the input image that serves as the content image\n","    @param style_image_path: Path to the input image that serves as the style image\n","    @param output_file_path: Path to which the style transfer output is to be stored \n","\"\"\"\n","def neural_style_transfer_vanilla(content_image_path:str, style_image_path:str, output_file_path:str) -> Optional[str]:\n","    # Nested function to load a PIL image as a TF tensor\n","    def load_img(path_to_img):\n","        max_dim = 512\n","        img = tf.io.read_file(path_to_img)\n","        img = tf.image.decode_image(img, channels=3)\n","        img = tf.image.convert_image_dtype(img, tf.float32)\n","\n","        shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n","        long_dim = max(shape)\n","        scale = max_dim / long_dim\n","\n","        new_shape = tf.cast(shape * scale, tf.int32)\n","\n","        img = tf.image.resize(img, new_shape)\n","        img = img[tf.newaxis, :]\n","        return img\n","    \n","    # Nested function to convert TF tensor back to PIL image\n","    def tensor_to_image(tensor):\n","        tensor = tensor*255\n","        tensor = np.array(tensor, dtype=np.uint8)\n","        if np.ndim(tensor)>3:\n","            assert tensor.shape[0] == 1\n","            tensor = tensor[0]\n","        return Image.fromarray(tensor)\n","\n","    try:\n","        global magenta_model\n","        content_image_tensor = load_img(content_image_path)\n","        style_image_tensor = load_img(style_image_path)\n","        stylised_image_tensor = magenta_model(tf.constant(content_image_tensor), tf.constant(style_image_tensor))[0]\n","        stylised_image_pil = tensor_to_image(stylised_image_tensor)\n","        stylised_image_pil.save(output_file_path)\n","        return output_file_path\n","    except Exception as e:\n","        print(f\"Error while performing vanilla neural style transfer: {e}\")\n","        return None\n","    \n","\"\"\"\n","    @method neural_style_transfer_vanilla_stream\n","        Perform OvA (one-vs-all) Neural style transfer with a single content/style image and a directory of style/content images\n","    @param single_image_path: Path to the single input image that serves as the content image\n","    @param stream_image_dir: Path to the directory containing the input images that serves as the content/style images\n","    @param ova_mode: Mode of neural style transfer. This parameter takes two values: \"style\" for single style image and a stream of content images; and \"content\" for a single content image and a stream of style images\n","    @param output_dir: Path to the parent directory where the contiguous melspectrogram images are to be stored\n","\"\"\"\n","def neural_style_transfer_vanilla_stream(single_image_path:str, stream_image_dir:str, ova_mode:str, output_dir:str) -> Optional[List[str]]:\n","    try:\n","        # Maintain a list of stream image filenames and count of generated images\n","        stream_image_filenames = glob(f\"{stream_image_dir}/*\")\n","        assert len(stream_image_filenames) > 0\n","\n","        # Keep running count of the current time index and number of images generated\n","        num_images_generated = 0\n","        saved_output_file_names = []\n","\n","        for stream_image in tqdm(stream_image_filenames):\n","            output_file = f\"{output_dir}/nst_{num_images_generated}.png\"\n","            # Check OvA mode\n","            if ova_mode == \"style\":\n","                output_file = neural_style_transfer_vanilla(content_image_path=stream_image,style_image_path=single_image_path,output_file_path=output_file)\n","                if output_file is not None:\n","                    saved_output_file_names.append(output_file)\n","                    num_images_generated += 1\n","\n","            if ova_mode == \"content\":\n","                output_file = neural_style_transfer_vanilla(content_image_path=single_image_path,style_image_path=stream_image,output_file_path=output_file)\n","                if output_file is not None:\n","                    saved_output_file_names.append(output_file)\n","                    num_images_generated += 1\n","\n","        return saved_output_file_names\n","\n","    except Exception as e:\n","        print(f\"Error while generating and saving neural style transfer image stream: {e}\")\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","    Collection of helper functions pertaining to the spectrogram domain of spectrogrand\n","\"\"\"\n","from typing import Optional, List\n","import librosa\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","import torch\n","from torchvision import transforms\n","torch.random.manual_seed(42)\n","DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","# from audio_helpers import load_wav_file, load_wav_chunk\n","\n","IDX_TO_LABEL_MAPPING = {0:'future house', 1:'bass house', 2:'progressive house', 3:'melodic house'}\n","\n","\"\"\"\n","    @method generate_and_save_melspectrogram\n","        Generate and save a melspectrogram for audio data \n","    @param input_data: Numpy array containing the audio to generate a spectrogram for\n","    @param input_sampling_rate: Sampling rate for the input audio\n","    @param output_file_path: Path to the output file where the generated melspectrogram is to be stored\n","    @param n_mels: Number of buckets used in the melspectrogram computation (default:128)\n","    @param hop_length: Hop length used in the melspectrogram computation (default: 512)\n","\"\"\"\n","def generate_and_save_melspectrogram(input_data:np.ndarray, input_sampling_rate:int, output_file_path:str, n_mels:int=128, hop_length:int=512) -> Optional[str]:\n","    try:\n","        # Generate melspectrogram\n","        melspectrum = librosa.feature.melspectrogram(\n","            y=input_data,\n","            sr=input_sampling_rate,\n","            hop_length= hop_length,\n","            window='hann',\n","            n_mels=n_mels\n","        )\n","        S_dB = librosa.power_to_db(melspectrum, ref=np.max)\n","\n","        # Save image\n","        img = librosa.display.specshow(S_dB, sr=input_sampling_rate)\n","        plt.savefig(output_file_path, bbox_inches=\"tight\",pad_inches=-0.1) # Removing whitespace ref: https://stackoverflow.com/questions/11837979/removing-white-space-around-a-saved-image \n","        return output_file_path\n","    except Exception as e:\n","        print(f\"Error while generating and saving melspectrogram: {e}\")\n","        return None\n","    \n","\"\"\"\n","    @method generate_and_save_melspectrogram_stream\n","        Generate and save contiguous melspectrograms given a parent audio file\n","    @param input_file_path: Path to the input audio file\n","    @param output_dir: Path to the parent directory where the contiguous melspectrogram images are to be stored\n","    @param chunk_duration: Length of the contiguous chunks of the audio (default:0.1)\n","\"\"\"\n","def generate_and_save_melspectrogram_stream(input_file_path:str, output_dir:str, chunk_duration:float=0.1) -> Optional[List[str]]:\n","    try:\n","        # Load the audio file to get its duration\n","        parent_sr, parent_data = load_wav_file(input_file_path=input_file_path)\n","        parent_duration = librosa.get_duration(y=parent_data,sr=parent_sr)\n","\n","        # Keep running count of the current time index and number of images generated\n","        num_images_generated = 0\n","        saved_output_file_names = []\n","\n","        current_chunk_time_start = 0.0\n","        current_chunk_time_end = current_chunk_time_start + chunk_duration\n","\n","        while float(current_chunk_time_end) <= float(parent_duration):\n","            output_file = f\"{output_dir}/melspec_{num_images_generated}.png\"\n","            # Load chunk data\n","            sr, y = load_wav_chunk(input_file_path=input_file_path,chunk_offset=current_chunk_time_start,chunk_duration=chunk_duration)\n","            # Construct melspectrogram\n","            output_file = generate_and_save_melspectrogram(input_data=y,input_sampling_rate=sr,output_file_path=output_file)\n","            if output_file is not None:\n","                saved_output_file_names.append(output_file)\n","                num_images_generated += 1\n","                \n","            current_chunk_time_start += chunk_duration\n","            current_chunk_time_end += chunk_duration\n","\n","        return saved_output_file_names\n","\n","    except Exception as e:\n","        print(f\"Error while generating and saving melspectrogram stream for {input_file_path}: {e}\")\n","        return None\n","\n","\"\"\"\n","    @method get_classified_genre\n","        Get the classified genre from a HouseX-trained model\n","    @param input_spectrogram_path: Path to the input spectrogram image\n","    @param model_path: Path to the `.pth` file containing the genre classification model\n","    @note The model will be loaded with the `torch.load(PATH)` moniker.\n","\"\"\"\n","def get_classified_genre(input_spectrogram_path:str, model_path:str) -> Optional[str]:\n","    try:\n","        global DEVICE, IDX_TO_LABEL_MAPPING\n","\n","        # Transform the input image into a torch tensor @ref: https://www.projectpro.io/recipes/convert-image-tensor-pytorch\n","        transform = transforms.Compose([\n","                transforms.Resize((96, 96)),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","\n","        img = Image.open(input_spectrogram_path).convert('RGB')\n","        transformed_img = transform(img=img)\n","        x = torch.Tensor(transformed_img)\n","        x = x.to(DEVICE)\n","\n","        # Load model\n","        model = torch.load(model_path)\n","        model.to(DEVICE)\n","        model.eval()\n","\n","        # Compute outputs\n","        with torch.no_grad():\n","            outputs = model(x.unsqueeze(0))\n","            y = torch.softmax(outputs, dim = 1).detach().cpu()\n","            selected_index = int(torch.argmax(y).item())\n","\n","        selected_genre = IDX_TO_LABEL_MAPPING[selected_index]\n","        return selected_genre\n","    except Exception as e:\n","        print(f\"Error while classifying genre of {input_spectrogram_path}: {e}\")\n","        return None"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Specify the topic of your creation here\n","\n","Example: `a futuristic spaceship`"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["PARENT_TOPIC = \"a futuristic spaceship\""]},{"cell_type":"markdown","metadata":{},"source":["### Generate candidate audio samples and choose the most novel one"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_inference_steps_custom = [600, 750]\n","saved_audio_files = create_and_save_audio_file_stream(topic = PARENT_TOPIC, output_dir = AUDIO_STREAM_DIR, num_inference_steps_list = num_inference_steps_custom, audio_length=10.0, negative_prompt = \"low quality, monotonous, boring\")\n","print(f\"Generated {len(saved_audio_files)} music samlples into {AUDIO_STREAM_DIR}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["GT_EMBEDDINGS_PATH = \"/kaggle/input/spectrogrand-public-release/kaggle-public-release/housex_ground_truth_embeddings.pkl\"\n","def calculate_novelty_score(input_file_path:str) -> float:\n","    try:\n","        sim_score = compute_clap_similarity(input_file_path=input_file_path, ground_truth_dict_path=GT_EMBEDDINGS_PATH, filter_genre=None)\n","        novelty_score = 1.0 - sim_score\n","        return novelty_score\n","    except Exception as e:\n","        print(f\"Error while computing novelty score: {e}\")\n","        return -1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["max_novelty_score = -1.0\n","selected_audio_file = None\n","\n","for _f in saved_audio_files:\n","    novelty_score = calculate_novelty_score(input_file_path=_f)\n","    if novelty_score > max_novelty_score:\n","        max_novelty_score = novelty_score\n","        selected_audio_file = _f \n","    \n","print(f\"Selecting {selected_audio_file} with the novelty score of {max_novelty_score}\")\n","\n","assert selected_audio_file is not None\n","assert max_novelty_score != -1.0"]},{"cell_type":"markdown","metadata":{},"source":["## Generate parent and stream spectrograms for the selected audio file and detect genre from the parent spectrogram"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sr, y = load_wav_file(input_file_path=selected_audio_file)\n","parent_spec_path = generate_and_save_melspectrogram(input_data=y, input_sampling_rate=sr, output_file_path=f\"{IMAGE_PARENT_DIR_SPEC}/parent.png\")\n","print(f\"Saved parent image spectrogram to {parent_spec_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["saved_chunks = generate_and_save_melspectrogram_stream(input_file_path=selected_audio_file, output_dir=IMAGE_STREAM_DIR_SPEC)\n","print(f\"Generated {len(saved_chunks)} music samples into {IMAGE_STREAM_DIR_SPEC}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["GENRE_CLF_MODEL_PATH = \"/kaggle/input/spectrogrand-public-release/kaggle-public-release/resnet_101_genre_classifier.pth\"\n","selected_genre = get_classified_genre(input_spectrogram_path=parent_spec_path,model_path=GENRE_CLF_MODEL_PATH)\n","print(f\"The selected audio is closest to the genre of {selected_genre}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Generate candidate album covers and choose the one with the best equi-weighted score of value and surprisingness"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sd_num_inference_steps = 200\n","saved_sd_images = generate_and_save_image_stream(genre_name=selected_genre,output_dir=IMAGE_STREAM_DIR_SD, num_inference_steps=sd_num_inference_steps, topic=PARENT_TOPIC)\n","print(f\"Generated {len(saved_sd_images)} image samples into {IMAGE_STREAM_DIR_SD}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["SURPRISE_EST_MODEL_PATH = \"/kaggle/input/spectrogrand-public-release/kaggle-public-release/mobilenet_surprise_estimation.pt\"\n","\n","def calculate_value_score(input_file_path:str) -> float:\n","    try:\n","        vila_score = get_vila_score(input_image_path=input_file_path)\n","        return vila_score\n","    except Exception as e:\n","        print(f\"Error while computing value score: {e}\")\n","        return -1.0\n","    \n","def calculate_surprise_score(input_file_path:str) -> float:\n","    try:\n","        surprise_score = get_surprise_score(input_image_path=input_file_path, model_path=SURPRISE_EST_MODEL_PATH)\n","        return surprise_score\n","    except Exception as e:\n","        print(f\"Error while computing surprise score: {e}\")\n","        return -1.0\n","    \n","def get_album_cover_score(input_file_path:str) -> float:\n","    try:\n","        value_score = calculate_value_score(input_file_path=input_file_path)\n","        surprise_score = calculate_surprise_score(input_file_path=input_file_path)\n","        if value_score == -1.0 or surprise_score == -1.0:\n","            return -1.0\n","        # Update the weights if your use case values value more than surprise or vice-versa\n","        return 0.5*value_score + 0.5*surprise_score\n","    except Exception as e:\n","        print(f\"Error while computing image score: {e}\")\n","        return -1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["max_image_score = -1.0\n","selected_image_file = None\n","\n","for _f in saved_sd_images:\n","    image_score = get_album_cover_score(input_file_path=_f)\n","    if image_score > max_image_score:\n","        max_image_score = image_score\n","        selected_image_file = _f \n","    \n","print(f\"Selecting {selected_image_file} with the equiweighted score of {max_image_score}\")\n","\n","assert selected_image_file is not None\n","assert max_image_score != -1.0"]},{"cell_type":"markdown","metadata":{},"source":["## Generate style transfer images with the spectrogram streams and chosen album cover image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ova_mode = \"content\"\n","style_transfer_outputs_single_content_image = neural_style_transfer_vanilla_stream(single_image_path=selected_image_file, output_dir=IMAGE_STREAM_DIR_NST_1, ova_mode=ova_mode, stream_image_dir=IMAGE_STREAM_DIR_SPEC)\n","print(f\"Generated {len(style_transfer_outputs_single_content_image)} style transferred images keeping {ova_mode} image constant\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ova_mode = \"style\"\n","style_transfer_outputs_single_style_image = neural_style_transfer_vanilla_stream(single_image_path=selected_image_file, output_dir=IMAGE_STREAM_DIR_NST_2, ova_mode=ova_mode, stream_image_dir=IMAGE_STREAM_DIR_SPEC)\n","print(f\"Generated {len(style_transfer_outputs_single_content_image)} style transferred images keeping {ova_mode} image constant\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f\"selected_image_file: {selected_image_file}\")\n","print(f\"BASE_DIR: {BASE_DIR}\")\n","print(f\"selected_audio_file: {selected_audio_file}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Create Videos using MoviePy\n","\n","⚠️ Note: For dependency reasons, you must restart the kernel and follow the steps listed below to use MoviePy. Please ensure all files are stored to `/kaggle/output`  \n","⚠️ Before you restart the kernel, make sure you copy and save the variables `{selected_image_file}`, `{BASE_DIR}`, and `{selected_audio_file}`\n","\n","For your convenience, you can paste them into this markdown cell:\n","```py\n","selected_image_file = \"/kaggle/working/logs_1709970288/saved_image_stream_sd/sdxl3.png\"\n","BASE_DIR = \"/kaggle/working/logs_1709970288\"\n","selected_audio_file = \"/kaggle/working/logs_1709970288/saved_audio_stream/audio1.wav\"\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install --upgrade ffmpeg moviepy\n","!pip install --upgrade decorator==4.0.2\n","!pip install moviepy --upgrade --force-reinstall"]},{"cell_type":"markdown","metadata":{},"source":["## Reload the variables saved from earlier"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["selected_image_file = \"/kaggle/working/logs_1709970288/saved_image_stream_sd/sdxl3.png\"\n","BASE_DIR = \"/kaggle/working/logs_1709970288\"\n","selected_audio_file = \"/kaggle/working/logs_1709970288/saved_audio_stream/audio1.wav\""]},{"cell_type":"markdown","metadata":{},"source":["## Load helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","    Collection of helper functions pertaining to the video domain of spectrogrand\n","\"\"\"\n","from moviepy.editor import AudioFileClip, ImageClip, ImageSequenceClip\n","from typing import Optional\n","\n","\"\"\"\n","    @method generate_and_save_video_dynamic\n","        Generate and save a dynamic video for a given audio file and image folder \n","    @param audio_file_path: Path to the background audio file\n","    @param image_dir_path: Path to the directory containing the list of images to be included in the video (@note Expected file format: *_{idx}.* - the files will be sorted based on `idx`.)\n","    @param output_video_path: Path to the file where the video is to be stored\n","\"\"\"\n","def generate_and_save_video_dynamic(audio_file_path:str, image_dir_path:str, output_video_path:str) -> Optional[str]:\n","    try:\n","        # Create audio clip\n","        with AudioFileClip(filename=audio_file_path) as audio_clip:\n","            audio_duration = audio_clip.duration\n","\n","            # Create image sequence clip\n","            with ImageSequenceClip(sequence=image_dir_path, fps = 10) as image_sequence_clip:\n","                image_sequence_clip = image_sequence_clip.set_duration(audio_duration)\n","                image_sequence_clip = image_sequence_clip.set_audio(audio_clip)\n","                image_sequence_clip = image_sequence_clip.set_fps(10)\n","\n","                # Export the clip\n","                image_sequence_clip.write_videofile(output_video_path)\n","                return output_video_path\n","    except Exception as e:\n","        print(f\"Error while generating dynamic video clip: {e}\")\n","        return None\n","\n","\"\"\"\n","    @method generate_and_save_video_static\n","        Generate and save a static video for a given audio file and image file \n","    @param audio_file_path: Path to the background audio file\n","    @param image_file_path: Path to the image file\n","    @param output_video_path: Path to the file where the video is to be stored\n","\"\"\"\n","def generate_and_save_video_static(audio_file_path:str, image_file_path:str, output_video_path:str) -> Optional[str]:\n","    try:\n","        # Create audio clip\n","        with AudioFileClip(filename=audio_file_path) as audio_clip:\n","            audio_duration = audio_clip.duration\n","\n","            # Create image clip\n","            with ImageClip(img=image_file_path) as image_clip:\n","                image_clip = image_clip.set_duration(audio_duration) # @note ref: https://stackoverflow.com/questions/75414756/combine-image-and-audio-together-using-moviepy-in-python\n","                image_clip = image_clip.set_audio(audio_clip)\n","                image_clip = image_clip.set_fps(10)\n","\n","                # Export the clip\n","                image_clip.write_videofile(output_video_path)\n","                return output_video_path\n","    except Exception as e:\n","        print(f\"Error while generating static video clip: {e}\")\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["VIDEOS_OUTUPUT_DIR = f\"{BASE_DIR}/saved_videos\"\n","IMAGE_STREAM_DIR_NST_1 = f\"{BASE_DIR}/saved_image_stream_nst_1\"\n","IMAGE_STREAM_DIR_NST_2 = f\"{BASE_DIR}/saved_image_stream_nst_2\"\n","\n","!mkdir -p {VIDEOS_OUTUPUT_DIR}"]},{"cell_type":"markdown","metadata":{},"source":["## Generate static video"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["static_video_path = generate_and_save_video_static(audio_file_path=selected_audio_file, image_file_path=selected_image_file, output_video_path=f\"{VIDEOS_OUTUPUT_DIR}/static.mp4\")\n","print(f\"Generated video at {static_video_path}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Generating dynamic videos"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dynamic_video_path_constant_content_image = generate_and_save_video_dynamic(audio_file_path=selected_audio_file, image_dir_path=IMAGE_STREAM_DIR_NST_1,output_video_path=f\"{VIDEOS_OUTUPUT_DIR}/dynamic1.mp4\")\n","print(f\"Generated video at {dynamic_video_path_constant_content_image}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dynamic_video_path_constant_style_image = generate_and_save_video_dynamic(audio_file_path=selected_audio_file, image_dir_path=IMAGE_STREAM_DIR_NST_2,output_video_path=f\"{VIDEOS_OUTUPUT_DIR}/dynamic2.mp4\")\n","print(f\"Generated video at {dynamic_video_path_constant_style_image}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4547642,"sourceId":7773126,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
