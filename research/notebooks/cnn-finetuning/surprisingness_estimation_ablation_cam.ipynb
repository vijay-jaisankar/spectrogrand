{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7446992,"sourceType":"datasetVersion","datasetId":4334648},{"sourceId":7773126,"sourceType":"datasetVersion","datasetId":4547642}],"dockerImageVersionId":30703,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Generating GRAD-CAM visualisations for Surprisingness estimation","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nall_test_images_ai = []\nall_test_images_human = []\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/mumu-image-classification-album/mumu-images-classification/test/ai'):\n    for filename in filenames:\n        all_test_images_ai.append(os.path.join(dirname, filename))\n\nfor dirname, _, filenames in os.walk('/kaggle/input/mumu-image-classification-album/mumu-images-classification/test/human'):\n    for filename in filenames:\n        all_test_images_human.append(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-24T04:19:07.859114Z","iopub.execute_input":"2024-04-24T04:19:07.859450Z","iopub.status.idle":"2024-04-24T04:19:07.869180Z","shell.execute_reply.started":"2024-04-24T04:19:07.859421Z","shell.execute_reply":"2024-04-24T04:19:07.868407Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"all_test_images_ai.sort(key = lambda x: x.split('/')[-1].replace('_SDXL.jpg', ''))\nall_test_images_ai[:5]","metadata":{"execution":{"iopub.status.busy":"2024-04-24T04:19:07.872474Z","iopub.execute_input":"2024-04-24T04:19:07.872781Z","iopub.status.idle":"2024-04-24T04:19:07.881458Z","shell.execute_reply.started":"2024-04-24T04:19:07.872757Z","shell.execute_reply":"2024-04-24T04:19:07.880575Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['/kaggle/input/mumu-image-classification-album/mumu-images-classification/test/ai/21yQYShp34LSDXL.jpg',\n '/kaggle/input/mumu-image-classification-album/mumu-images-classification/test/ai/313322Q12GLSDXL.jpg',\n '/kaggle/input/mumu-image-classification-album/mumu-images-classification/test/ai/314SCTT53VLSDXL.jpg',\n '/kaggle/input/mumu-image-classification-album/mumu-images-classification/test/ai/316Q04B445LSDXL.jpg',\n '/kaggle/input/mumu-image-classification-album/mumu-images-classification/test/ai/317QRZJXXALSDXL.jpg']"},"metadata":{}}]},{"cell_type":"code","source":"all_test_images_human.sort(key = lambda x: x.split('/')[-1].replace('.jpg', ''))\nall_test_images_human[:5]","metadata":{"execution":{"iopub.status.busy":"2024-04-24T04:19:07.882862Z","iopub.execute_input":"2024-04-24T04:19:07.883120Z","iopub.status.idle":"2024-04-24T04:19:07.891856Z","shell.execute_reply.started":"2024-04-24T04:19:07.883099Z","shell.execute_reply":"2024-04-24T04:19:07.891005Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['/kaggle/input/mumu-image-classification-album/mumu-images-classification/test/human/21yQYShp34L.jpg',\n '/kaggle/input/mumu-image-classification-album/mumu-images-classification/test/human/313322Q12GL.jpg',\n '/kaggle/input/mumu-image-classification-album/mumu-images-classification/test/human/314SCTT53VL.jpg',\n '/kaggle/input/mumu-image-classification-album/mumu-images-classification/test/human/316Q04B445L.jpg',\n '/kaggle/input/mumu-image-classification-album/mumu-images-classification/test/human/317QRZJXXAL.jpg']"},"metadata":{}}]},{"cell_type":"markdown","source":"## Load model","metadata":{}},{"cell_type":"code","source":"from typing import Optional, List\nfrom PIL import Image\nfrom io import BytesIO\nimport numpy as np\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torchvision import transforms\n\nMODEL_PATH = \"/kaggle/input/spectrogrand-public-release/kaggle-public-release/mobilenet_surprise_estimation.pt\"\nTORCH_DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n\n# Load the surprise estimation pipeline\nclass CreativeNet(nn.Module):\n    def __init__(self, train_baseline_classifier = False, num_output_classes = 2, dropout_rate = 0.20):\n        super().__init__()\n        \n        # Set instance variables\n        self.train_baseline_classifier = train_baseline_classifier\n        self.num_outuput_classes = num_output_classes\n        self.dropout_rate = dropout_rate\n        \n        # Set the current device for tensor calculations\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Baseline: MobileNet V3 small\n        self.baseline = models.mobilenet_v3_small(weights = models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n        \n        # Freeze the parameters of the base model (including but not limited to the last layers)\n        for param in self.baseline.parameters():\n            param.requires_grad = False\n        \n        if self.train_baseline_classifier:\n            for param in self.baseline.classifier.parameters():\n                param.requires_grad = True\n                \n        # Fully-connected block\n        self.fc1 = nn.Linear(1000, 128)\n        self.dropout1 = nn.Dropout(self.dropout_rate)\n        self.fc2 = nn.Linear(128, 32)\n        self.dropout2 = nn.Dropout(self.dropout_rate)\n        self.fc3 = nn.Linear(32, self.num_outuput_classes)\n        \n    def forward(self, x):\n        # Baseline\n        x = x.to(self.device)\n        x = self.baseline(x)\n        \n        # FC Block\n        x = F.leaky_relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.fc3(x))\n        x = torch.sigmoid(x)\n        return x\n    \nsurprise_model_args = {\n        \"train_baseline_classifier\" : False, \n        \"num_output_classes\" : 2,\n        \"dropout_rate\" : 0.35\n    }\n\n\"\"\"\n    @method get_surprise_score\n        Get the surprise coefficient from a MUMU-trained model\n    @param input_image_path: Path to the input SDXL image\n    @param model_path: Path to the `.pt` file containing the surprise estimation model\n    @note The model will be loaded with the `model.load_state_dict(torch.load(PATH))` moniker.\n\"\"\"\ndef get_surprise_score(input_image_path:str, model_path:str) -> Optional[str]:\n    try:\n        global TORCH_DEVICE, surprise_model   \n\n        # Transform the input image into a torch tensor @ref: https://www.projectpro.io/recipes/convert-image-tensor-pytorch\n        transform = transforms.Compose([\n                transforms.Resize((256, 256)),\n                transforms.ToTensor(),\n        ])\n\n        img = Image.open(input_image_path).convert(\"RGB\")\n        transformed_img = transform(img=img)\n        x = torch.Tensor(transformed_img)\n        x = x.to(TORCH_DEVICE)\n\n        # Load model\n        surprise_model.load_state_dict(torch.load(model_path))\n        surprise_model.to(TORCH_DEVICE)\n        surprise_model.eval()\n\n        # Compute outputs\n        with torch.no_grad():\n            outputs = surprise_model(x.unsqueeze(0))\n            y = torch.softmax(outputs, dim = 1).detach().cpu()\n            selected_score = float(y[0][1].item()) # Order of scores: ai, human\n        return selected_score\n    except Exception as e:\n        print(f\"Error while estimating surprise score of {input_image_path}: {e}\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-04-24T04:19:07.893134Z","iopub.execute_input":"2024-04-24T04:19:07.893414Z","iopub.status.idle":"2024-04-24T04:19:07.912877Z","shell.execute_reply.started":"2024-04-24T04:19:07.893391Z","shell.execute_reply":"2024-04-24T04:19:07.911901Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"surprise_model = CreativeNet(**surprise_model_args).to(TORCH_DEVICE)\nsurprise_model.load_state_dict(torch.load(MODEL_PATH))\nsurprise_model.to(TORCH_DEVICE)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T04:19:07.914216Z","iopub.execute_input":"2024-04-24T04:19:07.914676Z","iopub.status.idle":"2024-04-24T04:19:08.089990Z","shell.execute_reply.started":"2024-04-24T04:19:07.914642Z","shell.execute_reply":"2024-04-24T04:19:08.089163Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"CreativeNet(\n  (baseline): MobileNetV3(\n    (features): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (2): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (3): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (4): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (5): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (6): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (7): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (8): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (9): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (10): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (11): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (12): Conv2dNormActivation(\n        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=1)\n    (classifier): Sequential(\n      (0): Linear(in_features=576, out_features=1024, bias=True)\n      (1): Hardswish()\n      (2): Dropout(p=0.2, inplace=True)\n      (3): Linear(in_features=1024, out_features=1000, bias=True)\n    )\n  )\n  (fc1): Linear(in_features=1000, out_features=128, bias=True)\n  (dropout1): Dropout(p=0.35, inplace=False)\n  (fc2): Linear(in_features=128, out_features=32, bias=True)\n  (dropout2): Dropout(p=0.35, inplace=False)\n  (fc3): Linear(in_features=32, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Install GRAD-CAM package","metadata":{}},{"cell_type":"code","source":"!pip install -q grad-cam","metadata":{"execution":{"iopub.status.busy":"2024-04-24T04:19:08.091280Z","iopub.execute_input":"2024-04-24T04:19:08.091639Z","iopub.status.idle":"2024-04-24T04:19:20.629979Z","shell.execute_reply.started":"2024-04-24T04:19:08.091605Z","shell.execute_reply":"2024-04-24T04:19:20.628640Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Generating outputs","metadata":{}},{"cell_type":"code","source":"from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom torchvision.models import resnet50\nfrom PIL import Image\n\nfor i in range(10):\n\n    # Transform the input image into a torch tensor @ref: https://www.projectpro.io/recipes/convert-image-tensor-pytorch\n    transform = transforms.Compose([\n                    transforms.Resize((256, 256)),\n                    transforms.ToTensor(),\n    ])\n\n    img = Image.open(all_test_images_ai[i]).convert(\"RGB\")\n    transformed_img = transform(img=img)\n    x = torch.Tensor(transformed_img)\n    x = x.to(TORCH_DEVICE)\n\n    surprise_model.load_state_dict(torch.load(MODEL_PATH))\n    surprise_model.to(TORCH_DEVICE)\n\n    model = surprise_model.baseline\n    target_layers = [model.features[-2]]\n    # Note: input_tensor can be a batch tensor with several images!\n    input_tensor = x.unsqueeze(0)\n\n    # Construct the CAM object once, and then re-use it on many images:\n    cam = AblationCAM(model=model, target_layers=target_layers)\n\n    # You can also use it within a with statement, to make sure it is freed,\n    # In case you need to re-create it inside an outer loop:\n    # with GradCAM(model=model, target_layers=target_layers) as cam:\n    #   ...\n\n    # We have to specify the target we want to generate\n    # the Class Activation Maps for.\n    # If targets is None, the highest scoring category\n    # will be used for every image in the batch.\n    # Here we use ClassifierOutputTarget, but you can define your own custom targets\n    # That are, for example, combinations of categories, or specific outputs in a non standard model.\n\n    targets = [ClassifierOutputTarget(1)]\n\n    img = np.array(img)\n    resized_image =  img / 255.\n\n\n    with torch.enable_grad():\n        # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n\n        # In this example grayscale_cam has only one image in the batch:\n        grayscale_cam = grayscale_cam[0, :]\n        visualization = show_cam_on_image(resized_image.astype(np.float32), grayscale_cam, use_rgb=True)\n\n        # You can also get the model outputs without having to re-inference\n        model_outputs = cam.outputs\n    \n    img_save = Image.fromarray(visualization)\n    img_save.save(f\"sdxl_vis_human_label_{i}.png\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T04:19:20.631869Z","iopub.execute_input":"2024-04-24T04:19:20.632583Z","iopub.status.idle":"2024-04-24T04:19:22.077556Z","shell.execute_reply.started":"2024-04-24T04:19:20.632532Z","shell.execute_reply":"2024-04-24T04:19:22.076796Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 86.18it/s]\n100%|██████████| 3/3 [00:00<00:00, 83.60it/s]\n100%|██████████| 3/3 [00:00<00:00, 85.43it/s]\n100%|██████████| 3/3 [00:00<00:00, 83.83it/s]\n100%|██████████| 3/3 [00:00<00:00, 86.89it/s]\n100%|██████████| 3/3 [00:00<00:00, 86.09it/s]\n100%|██████████| 3/3 [00:00<00:00, 83.20it/s]\n100%|██████████| 3/3 [00:00<00:00, 85.30it/s]\n100%|██████████| 3/3 [00:00<00:00, 83.72it/s]\n100%|██████████| 3/3 [00:00<00:00, 86.56it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-04-24T04:19:22.078622Z","iopub.execute_input":"2024-04-24T04:19:22.078890Z","iopub.status.idle":"2024-04-24T04:19:23.060914Z","shell.execute_reply.started":"2024-04-24T04:19:22.078866Z","shell.execute_reply":"2024-04-24T04:19:23.059812Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"sdxl_vis_human_label_0.png  sdxl_vis_human_label_5.png\nsdxl_vis_human_label_1.png  sdxl_vis_human_label_6.png\nsdxl_vis_human_label_2.png  sdxl_vis_human_label_7.png\nsdxl_vis_human_label_3.png  sdxl_vis_human_label_8.png\nsdxl_vis_human_label_4.png  sdxl_vis_human_label_9.png\n","output_type":"stream"}]}]}